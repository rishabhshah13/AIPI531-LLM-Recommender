# AIPI 531 F23 Final Project - Item Recommendation using LLMs

## Overview

This repository contains the final project for AIPI 531 (Fall 2023), focusing on item recommendation using Language Model-based approaches. The primary objectives of the project are to teach Large Language Models (LLMs) for recommending items through prompt engineering and to compare their performance against a simple baseline recommender. The project leverages prompt engineering approaches outlined in [this reference](https://arxiv.org/pdf/2304.03153.pdf) while encouraging students to explore and improve upon these methods. The comparison is based on offline metrics, with an emphasis on creativity, completeness, and rigorous experimentation.

## Datasets

The primary dataset used in this project is MovieLens_100K. However, participants are encouraged to consider other datasets for item recommendations to enhance the diversity and applicability of the models.

## References and Tools

- [Hugging Face open source LLMs](https://huggingface.co/blog/llama2)
- [OpenAI LLMs](https://openai.com/chatgpt)
- [Prompt Engineering Guide](https://www.promptingguide.ai)
- [RAG: Retrieval-Augmented Generation](https://arxiv.org/pdf/2005.11401.pdf)
- [Langchain](https://python.langchain.com/docs/get_started/introduction)
- [Different ideas for applying LLMs for product recommendations](https://arxiv.org/pdf/2303.14524.pdf) ([Paper 2](https://arxiv.org/pdf/2305.02182.pdf), [Paper 3](https://arxiv.org/pdf/2304.10149.pdf))

## Repository Organization

The repository is organized as follows:

1. **Code**: Contains the implementation of LLM-based item recommenders, prompt engineering approaches, and baseline recommenders.
  
2. **Datasets**: Includes the MovieLens_100K dataset and documentation on how to incorporate additional datasets.

3. **Experiments**: Provides detailed information on the experiments conducted, including code, configurations, and results.

4. **Results**: Presents the results of the experiments, comparing the performance of LLMs and the baseline recommender based on various metrics.

5. **Docs**: Contains documentation for reproducing the results, running the code, and understanding the prompt engineering approaches.

## How to Reproduce Results

To reproduce the results, follow the steps outlined in the documentation provided in the **Docs** folder. Ensure that you have the necessary dependencies installed, and the datasets are set up appropriately.

## Summary of Findings

A brief summary of the key findings and insights derived from the experiments is provided in the **Results** section. It highlights the performance of LLMs compared to the baseline recommender and discusses any notable observations or improvements.

## Troubleshooting

For any questions or issues related to package installation or other technical difficulties, please contact the Teaching Assistant (TA) mentioned in the troubleshooting section. While the primary responsibility lies with the student, the TA is available to assist in resolving any installation issues.

---

*Note: This README serves as a guide for organizing the repository and providing essential information. It should be customized based on the actual content and outcomes of the project.*